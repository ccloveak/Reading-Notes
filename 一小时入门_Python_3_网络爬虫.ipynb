{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "一小时入门 Python 3 网络爬虫.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "1DeYSAXjR-rF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccloveak/Study_Notes/blob/master/%E4%B8%80%E5%B0%8F%E6%97%B6%E5%85%A5%E9%97%A8_Python_3_%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DeYSAXjR-rF",
        "colab_type": "text"
      },
      "source": [
        "#网络爬虫简介"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCWRm4-1SA_9",
        "colab_type": "text"
      },
      "source": [
        "网络爬虫，也叫网络蜘蛛(Web Spider)，根据网页地址(URL)爬取网页内。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa3OrhkmSBOE",
        "colab_type": "text"
      },
      "source": [
        "###审查元素  \n",
        "浏览器就是作为客户端从服务器端获取信息，然后将信息解析，并展示。可以在本地修改HTML信息，修改的信息不会回传到服务器，服务器存储的HTML信息不会改变。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HghJaD7SBRn",
        "colab_type": "text"
      },
      "source": [
        "###  简单实例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3rjdvxCSBUR",
        "colab_type": "text"
      },
      "source": [
        "*   urllib.request\n",
        "*   requests  [github](https://github.com/requests/requests)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQUcYk8zTvEw",
        "colab_type": "text"
      },
      "source": [
        "##### requests安装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyLlQO-MT1Wv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "6114a9b3-381b-4948-84aa-7c5f0b615488"
      },
      "source": [
        "!pip install requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.3.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1wGJzCEUB0J",
        "colab_type": "text"
      },
      "source": [
        "##### 简单实例  \n",
        "![基础方法](http://images.gitbook.cn/b30af580-9705-11e7-9b63-61d45538243e)  \n",
        "[官方中文教程](http://docs.python-requests.org/zh_CN/latest/user/quickstart.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFpZHRRUUgyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "if __name__ == '__main__':\n",
        "    target = 'http://gitbook.cn/'\n",
        "    req = requests.get(url=target)\n",
        "    print(req.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXcrHt08Vd3w",
        "colab_type": "text"
      },
      "source": [
        "*   requests.get()方法必须设置的一个参数就是url，因为我们得告诉GET请求，我们的目标是谁，我们要获取谁的信息\n",
        "*   将GET请求获得的响应内容存放到req变量中，然后使用req.text就可以获得HTML信息\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v98yJ6uXU2P2",
        "colab_type": "text"
      },
      "source": [
        "### 爬虫实战\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYlisepUWsgG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   用requests获取整个网页的HTML信息\n",
        "2.   解析HTML信息，提取感兴趣的内容 [Beautiful Soup](http://beautifulsoup.readthedocs.io/zh_CN/latest/)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    import requests\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        target = 'http://www.biqukan.com/1_1094/5403177.html'\n",
        "        req = requests.get(url=target)\n",
        "        print(req.text)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "```\n",
        "    from bs4 import BeautifulSoup\n",
        "    import requests\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        target = 'http://www.biqukan.com/1_1094/5403177.html'\n",
        "        req = requests.get(url = target)\n",
        "        html = req.text\n",
        "        bf = BeautifulSoup(html)\n",
        "        texts = bf.find_all('div', class_ = 'showtxt')\n",
        "        print(texts)\n",
        "```\n",
        "\n",
        "在解析html之前，我们需要创建一个Beautiful Soup对象。BeautifulSoup函数里的参数就是我们已经获得的html信息。然后我们使用find_all方法，获得html信息中所有class属性为showtxt的div标签。  \n",
        "\n",
        "find_all方法的第一个参数是获取的标签名，第二个参数class_是标签的属性，为什么不是class，而带了一个下划线呢？因为python中class是关键字，为了防止冲突，这里使用class_表示标签的class属性，class_后面跟着的showtxt就是属性值了。\n",
        "\n",
        "```\n",
        "<div id=\"content\", class=\"showtxt\">     \n",
        "\n",
        "```\n",
        "\n",
        "3.  去除div标签名，br标签，以及各种空格\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from bs4 import BeautifulSoup\n",
        "    import requests\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        target = 'http://www.biqukan.com/1_1094/5403177.html'\n",
        "        req = requests.get(url = target)\n",
        "        html = req.text\n",
        "        bf = BeautifulSoup(html)\n",
        "        texts = bf.find_all('div', class_ = 'showtxt')\n",
        "        print(texts[0].text.replace('\\xa0'*8,'\\n\\n'))\n",
        "```\n",
        "\n",
        "find_all匹配的返回的结果是一个列表。  \n",
        "提取匹配结果后，使用text属性，提取文本内容，滤除br标签。  \n",
        "随后使用replace方法，剔除空格，替换为回车进行分段。  \n",
        "&nbsp;在html中是用来表示空格的。  \n",
        "`replace('\\xa0'*8,'\\n\\n')`就是去掉下图的八个空格符号，并用回车代替\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW049O3_Zz60",
        "colab_type": "text"
      },
      "source": [
        "**分析下小说目录**  \n",
        "小说每章的链接放在了class属性为listmain的`<div>`标签下的`<a>`标签中。链接具体位置放在`html->body->div->dl->dd->a`的href属性中  \n",
        "先匹配class属性为`listmain的<div>`标签，再匹配`<a>`标签。\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    from bs4 import BeautifulSoup\n",
        "    import requests\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        target = 'http://www.biqukan.com/1_1094/'\n",
        "        req = requests.get(url = target)\n",
        "        html = req.text\n",
        "        div_bf = BeautifulSoup(html)\n",
        "        div = div_bf.find_all('div', class_ = 'listmain')\n",
        "        print(div[0])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB0y43QxatGF",
        "colab_type": "text"
      },
      "source": [
        "匹配每一个`<a>`标签，并提取章节名和章节文章  \n",
        "\n",
        "\n",
        "*   对Beautiful Soup返回的匹配结果a，使用a.get('href')方法就能获取href的属性值\n",
        "*   使用a.string就能获取章节名\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from bs4 import BeautifulSoup\n",
        "    import requests\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        server = 'http://www.biqukan.com/'\n",
        "        target = 'http://www.biqukan.com/1_1094/'\n",
        "        req = requests.get(url = target)\n",
        "        html = req.text\n",
        "        div_bf = BeautifulSoup(html)\n",
        "        div = div_bf.find_all('div', class_ = 'listmain')\n",
        "        a_bf = BeautifulSoup(str(div[0]))\n",
        "        a = a_bf.find_all('a')\n",
        "        for each in a:\n",
        "            print(each.string, server + each.get('href'))\n",
        "```\n",
        "为find_all返回的是一个列表，里边存放了很多的`<a>`标签，所以使用for循环遍历每个`<a>`标签并打印出来\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKGLFsdfbVrV",
        "colab_type": "text"
      },
      "source": [
        "##### 整合代码  \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from bs4 import BeautifulSoup\n",
        "    import requests, sys\n",
        "\n",
        "    \"\"\"\n",
        "    类说明:下载《笔趣看》网小说《一念永恒》\n",
        "    Parameters:\n",
        "        无\n",
        "    Returns:\n",
        "        无\n",
        "    Modify:\n",
        "        2017-09-13\n",
        "    \"\"\"\n",
        "    class downloader(object):\n",
        "\n",
        "        def __init__(self):\n",
        "            self.server = 'http://www.biqukan.com/'\n",
        "            self.target = 'http://www.biqukan.com/1_1094/'\n",
        "            self.names = []         #存放章节名\n",
        "            self.urls = []          #存放章节链接\n",
        "            self.nums = 0           #章节数\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:获取下载链接\n",
        "        Parameters:\n",
        "            无\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-13\n",
        "        \"\"\"\n",
        "        def get_download_url(self):\n",
        "            req = requests.get(url = self.target)\n",
        "            html = req.text\n",
        "            div_bf = BeautifulSoup(html)\n",
        "            div = div_bf.find_all('div', class_ = 'listmain')\n",
        "            a_bf = BeautifulSoup(str(div[0]))\n",
        "            a = a_bf.find_all('a')\n",
        "            self.nums = len(a[15:])                             #剔除不必要的章节，并统计章节数\n",
        "            for each in a[15:]:\n",
        "                self.names.append(each.string)\n",
        "                self.urls.append(self.server + each.get('href'))\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:获取章节内容\n",
        "        Parameters:\n",
        "            target - 下载连接(string)\n",
        "        Returns:\n",
        "            texts - 章节内容(string)\n",
        "        Modify:\n",
        "            2017-09-13\n",
        "        \"\"\"\n",
        "        def get_contents(self, target):\n",
        "            req = requests.get(url = target)\n",
        "            html = req.text\n",
        "            bf = BeautifulSoup(html)\n",
        "            texts = bf.find_all('div', class_ = 'showtxt')\n",
        "            texts = texts[0].text.replace('\\xa0'*8,'\\n\\n')\n",
        "            return texts\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:将爬取的文章内容写入文件\n",
        "        Parameters:\n",
        "            name - 章节名称(string)\n",
        "            path - 当前路径下,小说保存名称(string)\n",
        "            text - 章节内容(string)\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-13\n",
        "        \"\"\"\n",
        "        def writer(self, name, path, text):\n",
        "            write_flag = True\n",
        "            with open(path, 'a', encoding='utf-8') as f:\n",
        "                f.write(name + '\\n')\n",
        "                f.writelines(text)\n",
        "                f.write('\\n\\n')\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        dl = downloader()\n",
        "        dl.get_download_url()\n",
        "        print('《一年永恒》开始下载：')\n",
        "        for i in range(dl.nums):\n",
        "            dl.writer(dl.names[i], '一念永恒.txt', dl.get_contents(dl.urls[i]))\n",
        "            sys.stdout.write(\"  已下载:%.3f%%\" %  float(i/dl.nums) + '\\r')\n",
        "            sys.stdout.flush()\n",
        "        print('《一年永恒》下载完成')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4XE-U3Pb7VX",
        "colab_type": "text"
      },
      "source": [
        "### 优美壁纸下载"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je_BSCgScAAV",
        "colab_type": "text"
      },
      "source": [
        "URL：https://unsplash.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpsgd0nZcD1-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   使用requeusts获取整个网页的HTML信息；\n",
        "\n",
        "2.    使用Beautiful Soup解析HTML信息，找到所有`<img>`标签，提取src属性，获取图片存放地址；\n",
        "\n",
        "\n",
        "3.   根据图片存放地址，下载图片。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPrjcpTeceyD",
        "colab_type": "text"
      },
      "source": [
        "使用抓包工具  [Fiddler](ttp://www.telerik.com/fiddler)\n",
        "使用方法：打开软件，然后用浏览器打开目标网站"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79mhXX__c833",
        "colab_type": "text"
      },
      "source": [
        "编程提取json数据  \n",
        "\n",
        "\n",
        "1.   获取整个json数据\n",
        "2.   分析json数据\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS5yR-OgdpTz",
        "colab_type": "text"
      },
      "source": [
        "反爬虫手段，验证Request Headers\n",
        "\n",
        "Accept、Accept-Encoding、Accept-Language、DPR、User-Agent、Viewport-Width、accept-version、Referer、x-unsplash-client、authorization、Connection、Host  \n",
        "\n",
        "\n",
        "*   User-Agent：这里面存放浏览器的信息。可以看到上图的参数值，它表示我是通过Windows的Chrome浏览器，访问的这个服务器。如果我们不设置这个参数，用Python程序直接发送GET请求，服务器接受到的User-Agent信息就会是一个包含python字样的User-Agent。如果后台设计者验证这个User-Agent参数是否合法，不让带Python字样的User-Agent访问，这样就起到了反爬虫的作用。这是一个最简单的，最常用的反爬虫手段。\n",
        "\n",
        "\n",
        "*   Referer：这个参数也可以用于反爬虫，它表示这个请求是从哪发出的。可以看到我们通过浏览器访问网站，这个请求是从`https://unsplash.com/`，这个地址发出的。如果后台设计者，验证这个参数，对于不是从这个地址跳转过来的请求一律禁止访问，这样就也起到了反爬虫的作用。\n",
        "\n",
        "\n",
        "\n",
        "*   authorization：这个参数是基于AAA模型中的身份验证信息允许访问一种资源的行为。在我们用浏览器访问的时候，服务器会为访问者分配这个用户ID。如果后台设计者，验证这个参数，对于没有用户ID的请求一律禁止访问，这样就又起到了反爬虫的作用。\n",
        "\n",
        "\n",
        "\n",
        "Unsplash是根据authorization反爬虫  \n",
        "\n",
        "通过程序手动添加这个参数，然后再发送GET请求，就可以顺利访问了\n",
        "\n",
        "```\n",
        "    import requests\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        target = 'http://unsplash.com/napi/feeds/home'\n",
        "        headers = {'authorization':'your Client-ID'}\n",
        "        req = requests.get(url=target, headers=headers, verify=False)\n",
        "        print(req.text)\n",
        "```\n",
        "\n",
        "**以上为了获得json数据**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-RvRM7GemIB",
        "colab_type": "text"
      },
      "source": [
        "使用json.load()方法解析数据   获得 图片的ID已经\n",
        "\n",
        "通过字符串处理一下，就生成了我们需要的图片下载请求地址"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlCBkt_fhRyB",
        "colab_type": "text"
      },
      "source": [
        "**整合代码**  \n",
        "\n",
        "\n",
        "```\n",
        "import requests, json, time, sys\n",
        "    from contextlib import closing\n",
        "\n",
        "    class get_photos(object):\n",
        "\n",
        "        def __init__(self):\n",
        "            self.photos_id = []\n",
        "            self.download_server = 'https://unsplash.com/photos/xxx/download?force=trues'\n",
        "            self.target = 'http://unsplash.com/napi/feeds/home'\n",
        "            self.headers = {'authorization':'your Client-ID'}\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:获取图片ID\n",
        "        Parameters:\n",
        "            无\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-13\n",
        "        \"\"\" \n",
        "        def get_ids(self):\n",
        "            req = requests.get(url=self.target, headers=self.headers, verify=False)\n",
        "            html = json.loads(req.text)\n",
        "            next_page = html['next_page']\n",
        "            for each in html['photos']:\n",
        "                self.photos_id.append(each['id'])\n",
        "            time.sleep(1)\n",
        "            for i in range(4):\n",
        "                req = requests.get(url=next_page, headers=self.headers, verify=False)\n",
        "                html = json.loads(req.text)\n",
        "                next_page = html['next_page']\n",
        "                for each in html['photos']:\n",
        "                    self.photos_id.append(each['id'])\n",
        "                time.sleep(1)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:图片下载\n",
        "        Parameters:\n",
        "            无\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-13\n",
        "        \"\"\" \n",
        "        def download(self, photo_id, filename):\n",
        "            headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36'}\n",
        "            target = self.download_server.replace('xxx', photo_id)\n",
        "            with closing(requests.get(url=target, stream=True, verify = False, headers = self.headers)) as r:\n",
        "                with open('%d.jpg' % filename, 'ab+') as f:\n",
        "                    for chunk in r.iter_content(chunk_size = 1024):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "                            f.flush()\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        gp = get_photos()\n",
        "        print('获取图片连接中:')\n",
        "        gp.get_ids()\n",
        "        print('图片下载中:')\n",
        "        for i in range(len(gp.photos_id)):\n",
        "            print('  正在下载第%d张图片' % (i+1))\n",
        "            gp.download(gp.photos_id[i], (i+1))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aw8EO0nhiyj",
        "colab_type": "text"
      },
      "source": [
        "### 爱奇艺VIP视频下载"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZMIDFvIhqiN",
        "colab_type": "text"
      },
      "source": [
        "通过旋风视频VIP解析网站  URL：http://api.xfsub.com/  \n",
        "通用解析方式是： ` http://api.xfsub.com/index.php?url=[播放地址或视频id]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cYbnfYDh3G1",
        "colab_type": "text"
      },
      "source": [
        "使用Fiddler进行抓包\n",
        "\n",
        "\n",
        "1.   分析包内容\n",
        "2.   在抓包结果中，测试请求\n",
        "3.   分析json数据\n",
        "4.   得到视频格式的地址\n",
        "\n",
        "url有时效性 反爬虫策略也有时效性\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjTIdPqzh3J7",
        "colab_type": "text"
      },
      "source": [
        "编程思路\n",
        "\n",
        "\n",
        "1.   正则表达式匹配到key、time、url等信息。\n",
        "2.   根据匹配的到信息发POST请求，获得一个存放视频信息的url。\n",
        "3.   根据这个url获得视频存放的地址。\n",
        "4.   根据最终的视频地址，下载视频。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yns0jk-2h3Mx",
        "colab_type": "text"
      },
      "source": [
        "编写代码\n",
        "需要使用requests.session()保持我们的会话请求\n",
        "\n",
        "\n",
        "```\n",
        "import requests,re, json\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    class video_downloader():\n",
        "        def __init__(self, url):\n",
        "            self.server = 'http://api.xfsub.com'\n",
        "            self.api = 'http://api.xfsub.com/xfsub_api/?url='\n",
        "            self.get_url_api = 'http://api.xfsub.com/xfsub_api/url.php'\n",
        "            self.url = url.split('#')[0]\n",
        "            self.target = self.api + self.url\n",
        "            self.s = requests.session()\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:获取key、time、url等参数\n",
        "        Parameters:\n",
        "            无\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-18\n",
        "        \"\"\"\n",
        "        def get_key(self):\n",
        "            req = self.s.get(url=self.target)\n",
        "            req.encoding = 'utf-8'\n",
        "            self.info = json.loads(re.findall('\"url.php\",\\ (.+),', req.text)[0])    #使用正则表达式匹配结果，将匹配的结果存入info变量中\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:获取视频地址\n",
        "        Parameters:\n",
        "            无\n",
        "        Returns:\n",
        "            video_url - 视频存放地址\n",
        "        Modify:\n",
        "            2017-09-18\n",
        "        \"\"\"\n",
        "        def get_url(self):\n",
        "            data = {'time':self.info['time'],\n",
        "                'key':self.info['key'],\n",
        "                'url':self.info['url'],\n",
        "                'type':''}\n",
        "            req = self.s.post(url=self.get_url_api,data=data)\n",
        "            url = self.server + json.loads(req.text)['url']\n",
        "            req = self.s.get(url)\n",
        "            bf = BeautifulSoup(req.text,'xml')                                      #因为文件是xml格式的，所以要进行xml解析。\n",
        "            video_url = bf.find('file').string                                      #匹配到视频地址\n",
        "                return video_url\n",
        "\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        url = 'http://www.iqiyi.com/v_19rr7qhfg0.html#vfrm=19-9-0-1'\n",
        "        vd = video_downloader(url)\n",
        "        vd.get_key()\n",
        "        print(vd.get_url())\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3npvmYBh3Ph",
        "colab_type": "text"
      },
      "source": [
        "根据视频地址，使用urllib.request.urlretrieve()即可将视频下载下来\n",
        "\n",
        "\n",
        "```\n",
        "import requests,re, json, sys\n",
        "    from bs4 import BeautifulSoup\n",
        "    from urllib import request\n",
        "\n",
        "    class video_downloader():\n",
        "        def __init__(self, url):\n",
        "            self.server = 'http://api.xfsub.com'\n",
        "            self.api = 'http://api.xfsub.com/xfsub_api/?url='\n",
        "            self.get_url_api = 'http://api.xfsub.com/xfsub_api/url.php'\n",
        "            self.url = url.split('#')[0]\n",
        "            self.target = self.api + self.url\n",
        "            self.s = requests.session()\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:获取key、time、url等参数\n",
        "        Parameters:\n",
        "            无\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-18\n",
        "        \"\"\"\n",
        "        def get_key(self):\n",
        "            req = self.s.get(url=self.target)\n",
        "            req.encoding = 'utf-8'\n",
        "            self.info = json.loads(re.findall('\"url.php\",\\ (.+),', req.text)[0])    #使用正则表达式匹配结果，将匹配的结果存入info变量中\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:获取视频地址\n",
        "        Parameters:\n",
        "            无\n",
        "        Returns:\n",
        "            video_url - 视频存放地址\n",
        "        Modify:\n",
        "            2017-09-18\n",
        "        \"\"\"\n",
        "        def get_url(self):\n",
        "            data = {'time':self.info['time'],\n",
        "                'key':self.info['key'],\n",
        "                'url':self.info['url'],\n",
        "                'type':''}\n",
        "            req = self.s.post(url=self.get_url_api,data=data)\n",
        "            url = self.server + json.loads(req.text)['url']\n",
        "            req = self.s.get(url)\n",
        "            bf = BeautifulSoup(req.text,'xml')                                      #因为文件是xml格式的，所以要进行xml解析。\n",
        "            video_url = bf.find('file').string                                      #匹配到视频地址\n",
        "            return video_url\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:回调函数，打印下载进度\n",
        "        Parameters:\n",
        "            a b c - 返回信息\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-18\n",
        "        \"\"\"\n",
        "        def Schedule(self, a, b, c):\n",
        "            per = 100.0*a*b/c\n",
        "            if per > 100 :\n",
        "                per = 1\n",
        "            sys.stdout.write(\"  \" + \"%.2f%% 已经下载的大小:%ld 文件大小:%ld\" % (per,a*b,c) + '\\r')\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        \"\"\"\n",
        "        函数说明:视频下载\n",
        "        Parameters:\n",
        "            url - 视频地址\n",
        "            filename - 视频名字\n",
        "        Returns:\n",
        "            无\n",
        "        Modify:\n",
        "            2017-09-18\n",
        "        \"\"\"\n",
        "        def video_download(self, url, filename):\n",
        "            request.urlretrieve(url=url,filename=filename,reporthook=self.Schedule)\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        url = 'http://www.iqiyi.com/v_19rr7qhfg0.html#vfrm=19-9-0-1'\n",
        "        vd = video_downloader(url)\n",
        "        filename = '加勒比海盗5'\n",
        "        print('%s下载中:' % filename)\n",
        "        vd.get_key()\n",
        "        video_url = vd.get_url()\n",
        "        print('  获取地址成功:%s' % video_url)\n",
        "        vd.video_download(video_url, filename+'.mp4')\n",
        "        print('\\n下载完成！')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiinVoBfh3Sm",
        "colab_type": "text"
      },
      "source": [
        "urlretrieve()有三个参数\n",
        "\n",
        "1.   第一个url参数是视频存放的地址\n",
        "2.   第二个参数filename是保存的文件名\n",
        "3.   回调函数，它方便我们查看下载进度\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVRw9WUhh3VA",
        "colab_type": "text"
      },
      "source": [
        "###参考 \n",
        "[Github](https://github.com/Jack-Cherish/python-spider)"
      ]
    }
  ]
}