{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gitchat总结.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "5MIkRcKtL3iu",
        "mC6_I58ViDr9",
        "SvuNdRd2lXja",
        "f5OSe0Cn1i51",
        "srZ8-pfN2Enf",
        "VRvormSY2Etc",
        "im4uLLFl4D1p",
        "O-lqdb_z-h-x",
        "d0P3zKno-iGr",
        "Xc6mwEctC4dV",
        "B0LNpqTNC4rH",
        "lthZfVF4C4x4",
        "jfUUHqWIN5P4",
        "oLyMWl_gfhAe",
        "6pDsd-2vlY3c",
        "Pxs8wXrCqAnK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccloveak/Study_Notes/blob/master/gitchat%E6%80%BB%E7%BB%93.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MIkRcKtL3iu",
        "colab_type": "text"
      },
      "source": [
        "# 机器学习该如何入门？\n",
        ">https://gitbook.cn/gitchat/activity/59566e2374ea2f222cd663f0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNYOUWozMF_K",
        "colab_type": "text"
      },
      "source": [
        "###机器学习特点：\n",
        "\n",
        "1. 机器学习以数据为研究对象，是数据驱动的科学；\n",
        "\n",
        "2. 机器学习的目的是对数据进行预测与分析；\n",
        "\n",
        "3. 机器学习以模型方法为中心，利用统计学习的方法构建模型并且利用模型对未知数据进行预测和分析；\n",
        "\n",
        "4. 统计学习是概率论、统计学、信息论、计算理论、最优化理论以及计算机科学等多领域的交叉学科，并且逐渐形成自己独自的理论体系和方法论。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zijPle8VNERq",
        "colab_type": "text"
      },
      "source": [
        "### 机器学习对象\n",
        "机器学习研究的对象是多维向量空间的数据。它从各种不同类型的数据（数字，文本，图像，音频，视频）出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到数据的分析与预测中去。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPFu6zPNEZf",
        "colab_type": "text"
      },
      "source": [
        "### 机器学习分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM5RxH5LNEgj",
        "colab_type": "text"
      },
      "source": [
        "* 监督学习  监督学习是指进行训练的数据包含两部分信息：特征向量 + 类别标签。也就是说，他们在训练的时候每一个数据向量所属的类别是事先知道的。在设计学习算法的时候，学习调整参数的过程会根据类标进行调整，类似于学习的过程中被监督了一样，而不是漫无目标地去学习，故此得名。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX-W5frMNEk6",
        "colab_type": "text"
      },
      "source": [
        "* 无监督学习 相对于有监督而言，无监督方法的训练数据没有类标，只有特征向量。甚至很多时候我们都不知道总共的类别有多少个。因此，无监督学习就不叫做分类，而往往叫做聚类。就是采用一定的算法，把特征性质相近的样本聚在一起成为一类。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apX2JPabNEoY",
        "colab_type": "text"
      },
      "source": [
        "* 半监督学习 半监督学习是一种结合有监督学习和无监督学习的一种学习方式。它是近年来研究的热点，原因是在真正的模型建立的过程中，往往有类标的数据很少，而绝大多数的数据样本是没有确定类标的。这时候，我们无法直接应用有监督的学习方法进行模型的训练，因为有监督学习算法在有类标数据很少的情况下学习的效果往往很差。但是，我们也不能直接利用无监督学习的方式进行学习，因为这样，我们就没有充分的利用那些已给出的类标的有用信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CXEoXhzNEr6",
        "colab_type": "text"
      },
      "source": [
        "* 强化学习 所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号(强化信号)函数值最大，强化学习不同于连接主义学习中的监督学习，主要表现在教师信号上，强化学习中由环境提供的强化信号是对产生动作的好坏作一种评价(通常为标量信号)，而不是告诉强化学习系统RLS(reinforcement learning system)如何去产生正确的动作。由于外部环境提供的信息很少，RLS必须靠自身的经历进行学习。通过这种方式，RLS在行动-评价的环境中获得知识，改进行动方案以适应环境。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvQV96riNEvw",
        "colab_type": "text"
      },
      "source": [
        "### 机器学习的要素"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syi30_24NE7N",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   模型 其实就是机器学习训练的过程中所要学习的条件概率分布或者决策函数。\n",
        "*   策略 就是使用一种什么样的评价度量模型训练过程中的学习好坏的方法，同时根据这个方法去实施的调整模型的参数，以期望训练的模型将来对未知的数据具有最好的预测准确度。\n",
        "*   算法 算法是指模型的具体计算方法。它基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的计算方法去求解这个最优模型。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6l26OC2NE-l",
        "colab_type": "text"
      },
      "source": [
        "### 数学基础"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVUSa8V_P4Hf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   微积分\n",
        "\n",
        "*   线性代数 乘法 内积运算\n",
        "\n",
        "*   概率与统计 朴素贝叶斯算法\n",
        "\n",
        "\n",
        "*   经典算法学习 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L2U-AktQtlE",
        "colab_type": "text"
      },
      "source": [
        "#### 经典算法学习"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3afZY2nOQxTy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   分类算法： 逻辑回归（LR），朴素贝叶斯（Naive Bayes），支持向量机（SVM），随机森林（Random Forest），AdaBoost，GDBT，KNN，决策树……\n",
        "\n",
        "*   回归算法： 线性回归（Linear Regression），多项式回归（Polynomial Regression），逐步回归（Stepwise Regression），岭回归（Ridge Regression），套索回归（Lasso Regression）\n",
        "\n",
        "*   聚类算法： K均值（K-Means），谱聚类、DBSCAN聚类、模糊聚类、GMM聚类、层次聚\n",
        "\n",
        "*   降维算法： PCA（主成分分析）、SVD（奇异值分解）\n",
        "\n",
        "*   推荐算法： 协同过滤算法\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgdx6oIuRbjq",
        "colab_type": "text"
      },
      "source": [
        "### python库"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQlqHqi8RdYk",
        "colab_type": "text"
      },
      "source": [
        "*  scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nY0E3FWXKG6",
        "colab_type": "text"
      },
      "source": [
        "# 手把手教你做实时活体检测系统\n",
        "> https://gitbook.cn/books/5d1d4ccb72a3294c96090753/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yG2Smp2XQpP",
        "colab_type": "text"
      },
      "source": [
        "*  先看怎么使用colab 在colab上进行模型调整 争取9月前看完这篇文章"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zskH77Ga2TR",
        "colab_type": "text"
      },
      "source": [
        "# 机械学习极简入门\n",
        "> https://gitbook.cn/gitchat/column/5ad70dea9a722231b25ddbf8?columnId=5ad70dea9a722231b25ddbf8&refresh=true&newBuyer=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA6SyFxxa-wq",
        "colab_type": "text"
      },
      "source": [
        "## 课程大纲\n",
        "\n",
        "1.   绪论\n",
        "\n",
        "2.   基本原理\n",
        "\n",
        "3.   有监督学习 I\n",
        "\n",
        "4.   有监督学习 II\n",
        "\n",
        "5.   无监督学习\n",
        "\n",
        "6.   从机器学习到深度学习\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6H_HckFfZOu",
        "colab_type": "text"
      },
      "source": [
        "## 机器学习常用微积分知识速查手册\n",
        "> http://gitbook.cn/books/59ee907516fc0231837614e3/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDM-pxSKfc-t",
        "colab_type": "text"
      },
      "source": [
        "## 机器学习常用线性代数知识速查手册\n",
        "> http://gitbook.cn/books/59ed598e991df70ecd5a0049/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZOOQGVWfj9d",
        "colab_type": "text"
      },
      "source": [
        "## 公共 AI 支持库\n",
        "\n",
        "\n",
        "*   NumPy\n",
        "*   sklearn（scikit-learn）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC6_I58ViDr9",
        "colab_type": "text"
      },
      "source": [
        "## python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEny0SHKg8cv",
        "colab_type": "text"
      },
      "source": [
        "### python三种不同的运行方法：\n",
        "\n",
        "1.   在命令行直接运行\n",
        "\n",
        "\n",
        "2.   编写一个 Python 文件，将 print hello world 封装为一个函数，通过 main 函数调用它来运行；\n",
        "\n",
        "\n",
        "3.   编写一个 class，将 print hello world 封装为一个 method，通过 main 函数创建 class 实例来运行 method。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP-mOPFChbqz",
        "colab_type": "text"
      },
      "source": [
        "### 编写程序练习文件读写，文件和目录操作\n",
        "*  学会将 tsv、csv 之类的文件读入 array、list、dict 等结构，以及将这些变量打印到文本文件中的方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvuNdRd2lXja",
        "colab_type": "text"
      },
      "source": [
        "## 机器学习三要素之数据、模型、算法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baaBDEt9ljnQ",
        "colab_type": "text"
      },
      "source": [
        "### 数据\n",
        "\n",
        "*   构建一个向量空间模型（Vector Space Model，VSM）。VSM 负责将格式（文字、图片、音频、视频）转化为一个个向量\n",
        "*   无标注数据\n",
        "*   有标注数据\n",
        "*   特征工程  确定用哪些特征来表示数据； 确定用什么方式表达这些特征\n",
        "*   VSM 转换\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5tcv8ZNmdk-",
        "colab_type": "text"
      },
      "source": [
        "### 模型\n",
        "\n",
        "*   模型是机器学习的结果，这个学习过程，称为训练（Train）\n",
        "\n",
        "*   一个已经训练好的模型，可以被理解成一个函数： y=f(x)。\n",
        "\n",
        "*   模型是基于数据，经由训练得到的。\n",
        "\n",
        "*   训练就是：根据已经被指定的 f(x) 的具体形式——模型类型，结合训练数据，计算出其中各个参数的具体取值的过程。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOdQd5Bjmj_5",
        "colab_type": "text"
      },
      "source": [
        "### 算法\n",
        "\n",
        "*   有监督学习\n",
        "*   无监督学习\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqWLjk_crUTX",
        "colab_type": "text"
      },
      "source": [
        "####  监督学习"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ43ff2CrWzZ",
        "colab_type": "text"
      },
      "source": [
        "*   有监督学习的目标就是：让训练数据的所有 x 经过 f(x) 计算后，获得的 y’ 与它们原本对应的 y 的差别尽量小。\n",
        "\n",
        "*   用一个函数来描述 y’ 与 y 之间的差别，这个函数叫做损失函数（Loss Function）L（y, y’）= L(y, f(x))。\n",
        "\n",
        "*   Loss 函数针对一个训练数据，对于所有的训练数据，我们用代价函数（Cost Function）来描述整体的损失\n",
        "\n",
        "*   代价函数一般写作：J（theta）——注意，代价函数的自变量不再是 y 和 f(x)，而是变成了 theta，theta 表示 f(x) 中所有待定的参数（theta 也可以是一个向量，每个维度表示一个具体的参数）\n",
        "\n",
        "*   J（theta）的取值代表了整个模型付出的代价，这个代价自然是越小越好。\n",
        "\n",
        "*   具体的优化算法： 梯度下降法（Gradient Descent）、共轭梯度法（Conjugate Gradient）、牛顿法和拟牛顿法、模拟退火法（Simulated Annealing）\n",
        "\n",
        "*   能够决定有监督模型质量的，不是高深的算法和精密的模型，而是高质量的标注数据\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5OSe0Cn1i51",
        "colab_type": "text"
      },
      "source": [
        "## 获取模型的过程"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8yvNKkt1jEv",
        "colab_type": "text"
      },
      "source": [
        "### Step-1：数据准备"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mviacyec1jHw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   数据预处理：收集数据、清洗数据、标注数据\n",
        "\n",
        "\n",
        "2.   构建数据的向量空间模型（将文本、图片、音频、视频等格式的数据转换为向量）\n",
        "\n",
        "\n",
        "3.   将构建好向量空间模型的数据分为训练集、验证集和测试集\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDiEcEVL1jKq",
        "colab_type": "text"
      },
      "source": [
        "### Step-2：训练"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWWv4LZ91jNU",
        "colab_type": "text"
      },
      "source": [
        "*   训练集输入给训练程序，进行运算。训练程序的核心是算法，所有输入的向量化数据都会按该训练程序所依据的算法进行运算。训练程序输出的结果，就是模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9urVdhnK2EXG",
        "colab_type": "text"
      },
      "source": [
        "### Step-3：测试"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3PJiW-n2EZ4",
        "colab_type": "text"
      },
      "source": [
        "将测试集数据输入给训练获得的模型，得到预测结果；再将预测结果与这些数据原本预期的结果进行比较"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smjt0pg42EdS",
        "colab_type": "text"
      },
      "source": [
        "### 按一定规则计算模型质量的衡量指标\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xpOBbtO2Ekj",
        "colab_type": "text"
      },
      "source": [
        "比如 Precision、Recall、F1Score 等，根据指标的数值来衡量当前模型的质量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srZ8-pfN2Enf",
        "colab_type": "text"
      },
      "source": [
        "## 训练集、验证集和测试集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frI0hxvu2EqQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   训练集（Train Set）：用来做训练的数据的集合。\n",
        "\n",
        "*   验证集（Validation Set）：在训练的过程中，每个训练轮次结束后用来验证当前模型性能，为进一步优化模型提供参考的数据的集合。\n",
        "\n",
        "*   测试集（Test Set）：用来测试的数据的集合，用于检验最终得出的模型的性能。\n",
        "\n",
        "*   每个集合都应当是独立的，和另外两个没有重叠。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRvormSY2Etc",
        "colab_type": "text"
      },
      "source": [
        "## 训练的过程"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6glEwHn3XUa",
        "colab_type": "text"
      },
      "source": [
        "### 2.1编写训练程序"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqQ9afD_3XZE",
        "colab_type": "text"
      },
      "source": [
        "*   选择模型类型\n",
        "\n",
        "*   选择优化算法\n",
        "\n",
        "*   根据模型类型和算法编写程序\n",
        "\n",
        "\n",
        "*   列表项\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4_Xif_33XdN",
        "colab_type": "text"
      },
      "source": [
        "### 2.2训练 -> 获得临时模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3NfQZ-_3Xo7",
        "colab_type": "text"
      },
      "source": [
        "### 2.3在训练集上运行临时模型，获得训练集预测结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-R2aKlZ3Xru",
        "colab_type": "text"
      },
      "source": [
        "### 2.4在验证集上运行临时模型，获得验证集预测结果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAhZAVZt3X1J",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 综合参照 Step-2.3 和 Step-2.4 的预测结果，改进模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ote2fDaX3X35",
        "colab_type": "text"
      },
      "source": [
        "### 2.6 Step-2.2 到 Step-2.5 反复迭代，直至获得让我们满意，或者已经无法继续优化的模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im4uLLFl4D1p",
        "colab_type": "text"
      },
      "source": [
        "## 改进模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FgWuDJy4D4z",
        "colab_type": "text"
      },
      "source": [
        "对照机器学习三要素，模型的优化可以从三个方面来进行：数据、算法和模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XdSZuRb4EFz",
        "colab_type": "text"
      },
      "source": [
        "### 数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-28PoL7i4EIZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   机器学习的模型质量往往和训练数据有直接的关系\n",
        "*   大量的高质量训练数据，是提高模型质量的最有效手段。\n",
        "\n",
        "*   于有监督学习而言，标注是一个痛点，通常我们可以用来训练的数据量相当有限\n",
        "*   在有限的数据上，对数据进行归一化（Normalization，又译作正规化、标准化）等操作。\n",
        "\n",
        "*   采用 Bootstrap 等采样方法处理有限的训练/测试数据，以达到更好的运算效果\n",
        "*   根据业务进行特征选取：从业务角度区分输入数据包含的特征，并理解这些特征对结果的贡献\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-D8ATB49HoM",
        "colab_type": "text"
      },
      "source": [
        "### 调参（算法）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4ZNouFn9LbF",
        "colab_type": "text"
      },
      "source": [
        "*  超参数是需要模型训练者自己来设置和调整的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDz0s1r09Lka",
        "colab_type": "text"
      },
      "source": [
        "*   调参本身有点像一个完整的 Project，需要经历:\n",
        "\n",
        "\n",
        "\n",
        "1.   制定目标\n",
        "\n",
        "2.   制定策略\n",
        "\n",
        "3.   执行\n",
        "\n",
        "4.   验证\n",
        "\n",
        "5.   调整策略 -> 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZhSmxz79Lpk",
        "colab_type": "text"
      },
      "source": [
        "### 模型类型选择"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soVK5y-L-h2r",
        "colab_type": "text"
      },
      "source": [
        "*   对于某个分类问题，Logistic Regression 不行，可以换 Decision Tree 或者 SVM 试试。\n",
        "*   DL 模型（CNN、DNN、RNN、LSTM 等等）训练数据不足的情况下，很可能性能更差。\n",
        "*   无论工具还是方法，选对的，别选贵的\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-lqdb_z-h-x",
        "colab_type": "text"
      },
      "source": [
        "##  分类模型评判指标： Precision、Recall 和 F1Score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaQOKgua-iBJ",
        "colab_type": "text"
      },
      "source": [
        "*   对于分类而言，最简单也是最常见的验证指标：精准率（Precision）和召回率（Recall），为了综合这两个指标并得出量化结果，又发明了 F1Score。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZz9VHz-iD4",
        "colab_type": "text"
      },
      "source": [
        "*   精准率：Precision=TP/（TP+FP）\n",
        "*   召回率：Recall=TP/（TP+FN）\n",
        "*   F1Score = 2*(Precision * Recall)/(Precision + Recall)\n",
        "\n",
        "\n",
        "*   P、R、F1Score 在分类问题中都是对某一个类而言的。\n",
        "*   假设这个模型总共可以分10个类，那么对于每一个类都有一套独立的 P、R、F1Score 的值。衡量模型整体质量，要综合看所有10套指标，而不是只看一套\n",
        "\n",
        "*   NOTE：这几个指标也可以用于 seq2seq 识别模型的评价。\n",
        "*   seq2seq 识别实际上可以看作是一种位置相关的分类。每一种实体类型都可以被看作一个类别，因此也就同样适用 P、R、F1Score 指标。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0P3zKno-iGr",
        "colab_type": "text"
      },
      "source": [
        "## 指标对应的是模型&数据集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQgO8DfCC4ay",
        "colab_type": "text"
      },
      "source": [
        "*   同样一套指标，用来衡量同一个模型在不同数据集上的预测成果，最后的分数值可能不同（几乎可以肯定不同，关键是差别大小）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc6mwEctC4dV",
        "colab_type": "text"
      },
      "source": [
        "## 模型的偏差和过拟合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc7mlXdfC4go",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   一个机器学习模型的质量问题，从对训练集样本拟合程度的角度，可以分为两类：欠拟合（Underfitting）和过拟合 （Overfitting）。\n",
        "*   bias、error 和 variance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0LNpqTNC4rH",
        "colab_type": "text"
      },
      "source": [
        "## 最常用的优化算法——梯度下降法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNZy8vWpC4tx",
        "colab_type": "text"
      },
      "source": [
        "*   每一个机器学习模型都有一个目标函数，而学习的目标，就是最小化目标函数。\n",
        "\n",
        "*   当我们已经获得了一个函数，最小化该函数其实就是，在其自变量取值范围内，找到使得因变量最小的那个自变量取值点。\n",
        "*   几个经典机器学习模型的目标函数都是凸函数，函数的凸性保证了其有最小值。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lthZfVF4C4x4",
        "colab_type": "text"
      },
      "source": [
        "### 凸函数\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XySc9T-INYh_",
        "colab_type": "text"
      },
      "source": [
        "*   某个向量空间的凸子集（区间）上的实值函数，如果在其定义域上的任意两点 ，有 f(tx + (1-t)y) <= tf(x) + (1-t)f(y)，则称其为该区间上的凸函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9bJu_tBNobD",
        "colab_type": "text"
      },
      "source": [
        "### 梯度下降法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxriwS8NvDo",
        "colab_type": "text"
      },
      "source": [
        "*  既然已经知道了学习的目标就是最小化目标函数的取值，而目标函数又是凸函数，那么学习的目标自然转化成了寻找某个凸函数的最小值。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJC4ohu3NvHE",
        "colab_type": "text"
      },
      "source": [
        "*   未来在应用中构建自己的目标函数，那么千万记得在直接应用任何优化算法之前，应该先确定它是凸函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g8lpv-NN5D-",
        "colab_type": "text"
      },
      "source": [
        "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
        "\n",
        "1.   随机取一个自变量的值  $x_0$\n",
        "\n",
        "2.   对应该自变量算出对应点的因变量值：f($x_0$)；\n",
        "\n",
        "3.   计算 f( $x_0$) 处目标函数 f(x) 的导数；\n",
        "\n",
        "4.   从 f($x_0$ ) 开始，沿着该处目标函数导数的反方向，按一个指定的步长 $α$，向前“走一步”，走到的位置对应自变量取值为 $x_1$)；\n",
        "\n",
        "\n",
        "5.   继续重复2-4，直至退出迭代（达到指定迭代次数，或 f(x) 近似收敛到最优解）。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au33iGalN5HG",
        "colab_type": "text"
      },
      "source": [
        "### 梯度下降的超参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue863EKxN5Kh",
        "colab_type": "text"
      },
      "source": [
        "*   步长是算法自己学习不出来的，它必须由外界指定。\n",
        "\n",
        "*   这种算法不能学习，需要人为设定的参数，就叫做超参数。\n",
        "\n",
        "*   步长参数$α$是梯度下降算法中非常重要的超参数。这个参数设置的大小如果不合适，很可能导致最终无法找到最小值点。\n",
        "*   如果目标函数不能确定只有一个极小值，而获得的模型结果又不令人满意时，就该考虑是否是在学习的过程中，优化算法进入了局部而非全局最小值。\n",
        "*   这种情况下，可以尝试几个不同的起始点。甚至尝试一下大步长，说不定反而能够跨出局部最小值点所在的凸域"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfUUHqWIN5P4",
        "colab_type": "text"
      },
      "source": [
        "## 线性回归——从模型函数到目标函数\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH-qToInN5Sf",
        "colab_type": "text"
      },
      "source": [
        "### 从数据反推公式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nC3dnpXN5Wa",
        "colab_type": "text"
      },
      "source": [
        "### 综合利用训练数据，拟合线性回归函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfqrS5cMUhV9",
        "colab_type": "text"
      },
      "source": [
        "### 线性回归的目标函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaDI-R-UhZW",
        "colab_type": "text"
      },
      "source": [
        "*   在将训练样本的 x 逐个带入后，得出的预测年薪 y’ = a + bx 与真实年薪 y 整体的差异最小。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huImPUfuUhc4",
        "colab_type": "text"
      },
      "source": [
        "*   具体的一个样本的 y 和 y’ 的差异用 $(y′−y)^2$来表示"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6njaVO2iUhfw",
        "colab_type": "text"
      },
      "source": [
        "*   衡量整体差距用 Cost Function  形式如下（其中 m 为样本的个数，在本例中 m 取值为6）\n",
        "$J(a,b) = \\frac{1}{2m}\\sum_{i=1}^{m}(y'^{(i)} - y^{(i)})^2 = \\frac{1}{2m}\\sum_{i=1}^{m}(a + bx^{(i)} - y^{(i)})^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k1vvVk4UhjD",
        "colab_type": "text"
      },
      "source": [
        "*   能够让因变量 J(a, b) 取值最小的自变量 a 和 b，就是最好的 a 和 b。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbeKZdQPUhmG",
        "colab_type": "text"
      },
      "source": [
        "### 线性"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Wt7VVneCRh",
        "colab_type": "text"
      },
      "source": [
        "*   线性回归模型是：利用线性函数对一个或多个自变量 （x 或 ($x_1, x_2, ... x_k$)和因变量（y）之间的关系进行拟合的模型。\n",
        "*  线性回归模型构建成功后，这个模型表现为线性函数的形式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IohDbExeCWT",
        "colab_type": "text"
      },
      "source": [
        "*   线性函数的定义是：一阶（或更低阶）多项式，或零多项式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-ubYJZeCZG",
        "colab_type": "text"
      },
      "source": [
        "*   如果有多个独立自变量，$y = f(x_1, x_2, ..., x_k)$ 的函数形式则是：$f(x_1, x_2, ..., x_k)=a+b_1x_1+b_2x_2+...+b_kx_k$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oh2h_I3e38g",
        "colab_type": "text"
      },
      "source": [
        "*   特征是一维的，线性模型在二维空间构成一条直线；特征是二维的，线性模型在三维空间中构成一个平面；若特征是三维的，则最终模型在四维空间中构成一个体，以此类推。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XvLEcue4DN",
        "colab_type": "text"
      },
      "source": [
        "### 用线性回归模型拟合非线性关系"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Z-bfJgfg4Q",
        "colab_type": "text"
      },
      "source": [
        "*    设 $X=(x_1,x_2)$ ,其中 $x_1 = x^2; x_2 = x$，有：$f(x_1,x_2)=a+b_1x^2+b_2x=a+b_1x_1+b_2x_2$\n",
        "\n",
        "*   这就相当于拟合了一条二阶多项式对应的曲线\n",
        "\n",
        "*   再设 $B = (b_1, b_2)$ 则：$f(X) = a + BX$\n",
        "\n",
        "*   我们只需要在二维向量空间里训练$f(X) = a + BX$\n",
        "\n",
        "\n",
        "*   列表项\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okhyw3VMfg9m",
        "colab_type": "text"
      },
      "source": [
        "## 线性回归——梯度下降法求解目标函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLyMWl_gfhAe",
        "colab_type": "text"
      },
      "source": [
        "### 斜率、导数和偏微分"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-nJ6w0efhDm",
        "colab_type": "text"
      },
      "source": [
        "*    梯度下降法，总结起来就是：从任意点开始，在该点对目标函数求导，沿着导数方向（梯度）“走”（下降）一个给定步长，如此循环迭代，直至“走”到导数为0的位置，则达到极小值。\n",
        "\n",
        "\n",
        "*   一元函数在某一点处沿 x 轴正方向的变化率称为导数。但如果是二元或更多元的函数（自变量维度 >=2），则某一点处沿某一维度坐标轴正方向的变化率称为偏导数。\n",
        "*   导数/偏导数表现的是变化率，而变化本身，用另一个概念来表示，这个概念就是微分（对应偏导数，二元及以上函数有偏微分）。\n",
        "\n",
        "\n",
        "*   (偏）导数是针对函数上的一个点而言的，是一个值。而（偏）微分则是一个函数，其中的每个点表达的是原函数上各点沿着（偏）导数方向的变化。\n",
        "*   (偏）微分就是沿着（偏）导数的方向，产生了一个无穷小的增量\n",
        "\n",
        "\n",
        "*   当求出了一个函数的（偏）微分函数后，将某个变量带入其中，得出的（偏）微分函数对应的函数值，就是原函数在该点处，对该自变量求导的导数值。\n",
        "*   只要求出了目标函数的（偏）微分函数，那么目标函数自变量值域内每一点的导数值也就都可以求了。\n",
        "\n",
        "*   最基本的求导规则，函数（整体，而非在一个点处）求导的结果，就是微分函数\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY73jGWjfhGZ",
        "colab_type": "text"
      },
      "source": [
        "#### 常用规则中最常用的几条求导规则\n",
        "\n",
        "1.   常数的导数是零：(c)' = 0；\n",
        "\n",
        "2.   x 的 n 次幂的导数是 n 倍的 x 的 n-1 次幂：$(x^n)' = nx^{n-1}$\n",
        "\n",
        "3.   对常数乘以函数求导，结果等于该常数乘以函数的导数：(cf)' = cf'；\n",
        "4.   两个函数 f 和 g 的和的导数为：(f+g)' = f' + g'；\n",
        "\n",
        "\n",
        "5.   两个函数 f 和 g 的积的导数为：(fg)' = f'g + fg'\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pDsd-2vlY3c",
        "colab_type": "text"
      },
      "source": [
        "### 梯度下降求解目标函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdzFpV03lY6W",
        "colab_type": "text"
      },
      "source": [
        "*  $ \\frac{\\partial{J(a,b)}}{\\partial{a}} = \\frac{1}{(m)}\\sum_{i=1}^{m}((a+bx^{(i)}) - y^{(i)})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWIo85pFlZKO",
        "colab_type": "text"
      },
      "source": [
        "*   $\\frac{\\partial{J(a,b)}}{\\partial{b}} = \\frac{1}{(m)}\\sum_{i=1}^{m}x^{(i)}((a+bx^{(i)}) - y^{(i)})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx8mUoMWlZPq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Step 1：任意给定 a 和 b 的初值。  a = 0; b = 0;\n",
        "\n",
        "*   Step 2：用梯度下降法求解 a 和 b，伪代码如下：  \n",
        "$repeat \\,\\, until\\,\\,  convergence \\{$\n",
        "\n",
        "$\\\\    \\hspace{1cm} a = a - \\alpha  \\frac{\\partial{J(a,b)}}{\\partial{a}}$  \n",
        "$ \\hspace{1cm} b = b - \\alpha  \\frac{\\partial{J(a,b)}}{\\partial{b}}$ $\\}$\n",
        "\n",
        "*   当下降的高度小于某个指定的阈值（近似收敛至最优结果），则停止下降。\n",
        "\n",
        "*   将上面展开的式子带入上面的代码，就是：\n",
        "$repeat \\,\\, until\\,\\, convergence \\{$  \n",
        "$\\hspace{1cm}sumA = 0$  \n",
        "$\\hspace{1cm}sumB = 0$  \n",
        "$\\hspace{1cm}for\\,\\, i = 1\\,\\, to\\,\\, m \\{$  \n",
        "$\\\\  \\hspace{2cm}sumA = sumA +  (a+bx^{(i)} - y^{(i)})$  \n",
        "$\\hspace{2cm}sumB = sumB + x^{(i)}(a+bx^{(i)} - y^{(i)})$    $\\}$  \n",
        "$\\hspace{1cm}a = a - \\alpha \\frac{sumA}{m}$  \n",
        "$\\hspace{1cm} b = b - \\alpha \\frac{sumB}{m}$  \n",
        "$\\}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy2MMDCzoaJO",
        "colab_type": "text"
      },
      "source": [
        "### 通用线性回归模型的目标函数求解"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr3p_uCfpvx9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   n 维自变量的线性回归模型对应的目标函数：\n",
        "$J(\\theta_0,\\theta_1, ..., \\theta_n) = \\frac{1}{(2m)}\\sum_{i=1}^{m} (y'^{(i)}-y^{(i)})^{2} = \\frac{1}{(2m)}\\sum_{i=1}^{m}(\\theta_0+\\theta_1x_1^{(i)}+\\theta_2x_2^{(i)} + ... + \\theta_n x_n^{(i)} -y^{(i)})^{2}$\n",
        "\n",
        "*   再设：$X=[x_0, x_1, x_2, ..., x_n] ，\\Theta = [\\theta_0, \\theta_1, \\theta_2, ..., \\theta_n]$\n",
        "*   $f(X) = \\Theta^{T}  X$  \n",
        "\n",
        "\n",
        "*   $h(X) = \\Theta^{T} X$\n",
        "\n",
        "\n",
        "*   相应的目标函数就是：\n",
        "$J(\\Theta) = \\frac{1}{(2m)}\\sum_{i=1}^{m}(h_\\theta(X^{(i)})-y^{(i)})^{2}$\n",
        "*   同样应用梯度下降，实现的过程是：\n",
        "$repeat\\,\\, until\\,\\, convergence \\{$  \n",
        "$\\\\    \\hspace{1cm} \\Theta \\:= \\Theta - \\alpha  \\frac{\\partial{J(\\Theta)}}{\\partial{\\Theta}}$  \n",
        "$\\}$  \n",
        "*   细化为针对 $theta_j $的形式就是：\n",
        "$repeat\\,\\, until\\,\\, convergence \\{$  \n",
        "$\\hspace{1cm}  for\\,\\, j = 1\\,\\, to\\,\\, n \\{$  \n",
        "$\\hspace{2cm} sum_j = 0$  \n",
        "$\\hspace{2cm}for\\,\\, i = 1\\,\\, to\\,\\, m\\{$  \n",
        "$\\hspace{3cm}sum_j = sum_j +  (\\theta_0 +\\theta_1x_1^{(i)}+\\theta_2x_2^{(i)}  + ... + \\theta_nx_n^{(i)} -y^{(i)})x_j^{(i)}$  \n",
        "$\\hspace{2cm}\\}$  \n",
        "$\\hspace{2cm} \\theta_j \\:= \\theta_j - \\alpha  \\frac{sum_j}{m}$  \n",
        "$\\hspace{1cm}\\}$  \n",
        "$\\}$  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFFhpsZqpv84",
        "colab_type": "text"
      },
      "source": [
        "### 线性回归的超参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlcaIE6Ls5F2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   对于线性回归而言，只要用到梯度下降，就会有步长参数 alpha 这个超参数。\n",
        "\n",
        "*   如果训练结果偏差较大，可以尝试调小步长；如果模型质量不错但是训练效率太低，可以适当放大步长；也可以尝试使用动态步长，开始步长较大，随着梯度的缩小，步长同样缩小\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thHqUhr9s5OW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   如果训练程序是通过人工指定迭代次数来确定退出条件，则迭代次数也是一个超参数。\n",
        "\n",
        "*   如果训练程序以模型结果与真实结果的整体差值小于某一个阈值为退出条件，则这个阈值就是超参数。\n",
        "*   在模型类型和训练数据确定的情况下，超参数的设置就成了影响模型最终质量的关键。\n",
        "\n",
        "\n",
        "*   在实际应用中，能够在调参方面有章法，而不是乱试一气，就有赖于大家对于模型原理和数据的掌握了。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34JSKE_EtSwI",
        "colab_type": "text"
      },
      "source": [
        "### 编写线性回归训练/预测程序\n",
        "*   很多现成的方法库，可以直接调用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni63-7SCthlF",
        "colab_type": "code",
        "outputId": "3a1b5725-a7cb-41cf-8257-08cd3be33adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "experiences = np.array([0,1,2,3,4,5,6,7,8,9,10])\n",
        "salaries = np.array([103100, 104900, 106800, 108700, 110400, 112300, 114200, 116100, 117800, 119700, 121600])\n",
        "\n",
        "    # 将特征数据集分为训练集和测试集，除了最后 4 个作为测试用例，其他都用于训练\n",
        "X_train = experiences[:7]\n",
        "X_train = X_train.reshape(-1,1)\n",
        "X_test = experiences[7:]\n",
        "X_test = X_test.reshape(-1,1)\n",
        "\n",
        "    # 把目标数据（特征对应的真实值）也分为训练集和测试集\n",
        "y_train = salaries[:7]\n",
        "y_test = salaries[7:]\n",
        "\n",
        "    # 创建线性回归模型\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "    # 用训练集训练模型——看就这么简单，一行搞定训练过程\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "    # 用训练得出的模型进行预测\n",
        "diabetes_y_pred = regr.predict(X_test)\n",
        "\n",
        "    # 将测试结果以图标的方式显示出来\n",
        "plt.scatter(X_test, y_test,  color='black')\n",
        "plt.plot(X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
        "\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEKCQAaAkCCQASAkCGQACIn/AwhFyYg7EC1TAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxs8wXrCqAnK",
        "colab_type": "text"
      },
      "source": [
        "## 朴素贝叶斯分类器——从贝叶斯定理到分类模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KF6k84CqDtu",
        "colab_type": "text"
      },
      "source": [
        "### 公式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5L9Y8OWqD28",
        "colab_type": "text"
      },
      "source": [
        "* $P(A|B) =\\frac{ P(B|A) P(A)}{P(B)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO6APoLxqEMX",
        "colab_type": "text"
      },
      "source": [
        "### 一般化的贝叶斯公式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Z3yz_MqEal",
        "colab_type": "text"
      },
      "source": [
        "*   $P(A_i|B) = \\frac{P(B|A_i) P(A_i)}{\\sum_{j}P(B|A_j)P(A_j)}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KPouPc7qD_6",
        "colab_type": "text"
      },
      "source": [
        "### 连续概率的贝叶斯定理的形式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yw94WFsqEEW",
        "colab_type": "text"
      },
      "source": [
        "*  $f(x|y) = \\frac{f(y|x)f(x)}{\\int_{-\\infty}^{\\infty}f(y|x)f(x)dx}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aINxfOfjqEIL",
        "colab_type": "text"
      },
      "source": [
        "### 朴素贝叶斯分类器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFmwfCsJZsS3",
        "colab_type": "text"
      },
      "source": [
        "*  “朴素贝叶斯”（Naïve Bayes）既可以是一种算法——朴素贝叶斯算法，也可以是一种模型——朴素贝叶斯分类模型（分类器)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFxLbRdhaGJ8",
        "colab_type": "text"
      },
      "source": [
        "### 朴素贝叶斯算法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9apRql8uaII3",
        "colab_type": "text"
      },
      "source": [
        "*  $P(A|b_1,b_2,…, b_n) =\\frac{1}{Z}P(A) \\prod_{i=1}^{n} P(b_i|A)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffe4RiAlaIU6",
        "colab_type": "text"
      },
      "source": [
        "### 一款极简单的朴素贝叶斯分类器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-g7rZpVcbIi",
        "colab_type": "text"
      },
      "source": [
        "*   $b_1$ 到 $b_2$\n",
        " 是特征（Feature），而 A 则是最终的类别（Class）\n",
        "*  $P(C|F_1,F_2,…, F_n) =\\frac{1}{Z}P(C) \\prod_{i=1}^{n} P(F_i|C)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnXVNkegczjO",
        "colab_type": "text"
      },
      "source": [
        "#### 预测顺序\n",
        "\n",
        "1.   有一个朴素贝叶斯分类模型（器），它能够区分出 k 个类 $（c_1, c_2, …, c_k）$,用来分类的特征有 n 个：$(F_1, F_2, …, F_n)$\n",
        "\n",
        "\n",
        "2.   现在有个样本 s，我们要用 NB 分类器对它做预测，则需要先提取出这个样本的所有特征值$F_1$到$F_n$,将其带入到下式中进行 k 次运算:$P(C=c_j)\\prod_{i=1}^{n}P(F_i=f_i|C=c_j)$\n",
        "\n",
        "\n",
        "3.   然后比较这 k 次的结果，选出使得运算结果达到最大值的那个 $c_j（j=1, 2, …, k）$\n",
        "—— 这个 $c_j$对应的类别就是预测值。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJWHxAIIcziJ",
        "colab_type": "text"
      },
      "source": [
        "*  在训练样本的基础上做一系列概率运算，然后用这些算出来的概率按朴素贝叶斯公式“拼装”成分类模型——这就成了朴素贝叶斯分类器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_dR7-GofjFu",
        "colab_type": "text"
      },
      "source": [
        "## 朴素贝叶斯分类器——条件概率的参数估计"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaRAgo_Jh4Zl",
        "colab_type": "text"
      },
      "source": [
        "*  通过该特征在数据样本中的分布来计算该特征的条件概率"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfP6HQdVh4iK",
        "colab_type": "text"
      },
      "source": [
        "### 明确符号\n",
        "\n",
        "*   $D：$表示训练集；\n",
        "\n",
        "*  $D_c：$表示训练集中最终分类结果为 $c$\n",
        " 的那部分样本的集合；\n",
        "\n",
        "*   $X：$表示一个训练样本（单个样本）；\n",
        "*  $x_i^{(j)}$表示第 $j$个样本的第 $i$ 个特征的特征值；\n",
        "\n",
        "\n",
        "*   $m：D$中样本的个数；\n",
        "\n",
        "\n",
        "*   $m_c:D_c$中样本的个数，一般情况下（$m_c < m$）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhnWYFlNh4lS",
        "colab_type": "text"
      },
      "source": [
        "### 假设"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMVPoXKgh4ox",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   $P(x_i | c)$具有特定的形式，这个具体的形式是事先就已经认定的，不需要求取\n",
        "*   数据集合 $D$，其中每个样本的第 $i$个特征都符合上述假定。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33CPy890h4s8",
        "colab_type": "text"
      },
      "source": [
        "### 目标\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5MBWJwrn-o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   利用 $D$求出 $\\theta_{c,i}$的值。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg_cEWo2sPNR",
        "colab_type": "text"
      },
      "source": [
        "###  高斯分布"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoTXLcforoN7",
        "colab_type": "text"
      },
      "source": [
        "*  $P(x_i |c)$ 符合高斯分布，则：$P(x_i|c) = \\frac{1}{{ \\sqrt {2\\pi \\sigma_{c,i}^2 } }}exp( \\frac{-(x_i - \\mu_{c,i})^2 }{{2\\sigma_{c,i} ^2 }})$\n",
        "\n",
        "*  斯分布又名正态分布（在二维空间内形成钟形曲线），每一个高斯分布由两个参数——均值和方差，即上式中的$\\mu_{c,i}$和 $\\sigma_{c,i}$  ——决定，也就是说 \n",
        "$\\theta_{c,i} = (\\mu_{c,i} ，\\sigma_{c,i} )$\n",
        "*   ![高斯分布](http://images.gitbook.cn/fefe1740-4779-11e8-9e8f-c159b4fd4ecb)\n",
        "\n",
        "*   $\\theta_{c,i}$是我们要以 $D$为训练数据，通过训练过程得到的结果。\n",
        "*   这个训练过程，要用到概率统计中参数估计（Parameter Estimation）的方法。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3KNUcrPxe2r",
        "colab_type": "text"
      },
      "source": [
        "### 极大似然估计 (Maximum Likelihood Estimation, MLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUauVxHfxkXT",
        "colab_type": "text"
      },
      "source": [
        "#### 参数估计的常用策略"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOTAMty_xmuX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   先假定样本特征具备某种特定的概率分布形式\n",
        "2.   再基于训练样本对特征的概率分布参数进行估计\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Q3dPYmyavp",
        "colab_type": "text"
      },
      "source": [
        "### 似然（Likelihood）\n",
        "\n",
        "\n",
        "*   似然指某种事件发生的可能，和概率相似\n",
        "*   率用在已知参数的情况下，用来预测后续观测所得到的结果。似然则正相反，用于参数未知，但某些观测所得结果已知的情况，用来对参数进行估计。\n",
        "*   极大似然估计，就是去寻找让似然函数 $L(\\theta_{c,i})$的取值达到最大的参数值的估计方法。\n",
        "*   参数 $\\theta_{c,i}$的似然函数记作$L(\\theta_{c,i})$,它表示了$D_c$中的$m_c$个样本 \n",
        "$X_1，X_2，… X_{m_c}$ 在第 $i$个特征上的联合概率分布：$L(\\theta_{c,i}) =  \\prod_{j=1}^{m_c} P(x_i^{(j)}|\\theta_{c,i})$\n",
        "![似然函数示意图](http://images.gitbook.cn/ff774010-4793-11e8-b473-a34c15316faa)\n",
        "*   最大化一个似然函数同最大化它的自然对数是等价的\n",
        "*   对似然函数求导，然后在设定导函数为0的情况下，求取$\\theta_{c,i}$的最大值——$\\theta_{c,i}^*$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqs1Sh091Q8L",
        "colab_type": "text"
      },
      "source": [
        "### 正态分布的极大似然估计\n",
        "\n",
        "*   $P(x_i | c)$为正态分布。此时，我们有：$LL(\\theta_{c,i}) =  LL(\\mu_{c,i}, \\sigma_{c,i}^2) = \\sum_{j=1}^{m_c} log(\\frac{1}{{ \\sqrt {2\\pi \\sigma_{c,i}^2 } }}exp( \\frac{-(x_i^{(j)} - \\mu_{c,i})^2 }{{2\\sigma_{c,i} ^2 }}))$\n",
        "\n",
        "\n",
        "*   把$\\sigma_{c,i}^2$看作一个独立参数，即：\n",
        "$\\theta_{c,i} = (\\mu_{c,i}, \\sigma_{c,i} ^ 2)$\n",
        "\n",
        "*   最后得出：$\\sigma_{c,i}^2 = \\frac{1}{m_c} \\sum_{j=1}^{m_c}(x_i^{(j)} - \\mu_{c,i})^2$\n",
        "\n",
        "*   估算出了每一个特征的$\\theta_{c,i}$ 之后，再将所有求出了具体参数的分布带回到朴素贝叶斯公式中，生成朴素贝叶斯分类器，再用来做预测。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyokDsB2R3B",
        "colab_type": "text"
      },
      "source": [
        "### 用代码实现朴素贝叶斯模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDmnWjra2wX2",
        "colab_type": "code",
        "outputId": "718490f8-ba46-4a56-e179-b2c7b399e31f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Importing dataset. \n",
        "# Please refer to the 【Data】 part after the code for the data file.\n",
        "data = pd.read_csv(\"career_data.csv\") \n",
        "# Convert categorical variable to numeric\n",
        "data[\"985_cleaned\"]=np.where(data[\"985\"]==\"Yes\",1,0)\n",
        "data[\"education_cleaned\"]=np.where(data[\"education\"]==\"bachlor\",1,\n",
        "                                  np.where(data[\"education\"]==\"master\",2,\n",
        "                                           np.where(data[\"education\"]==\"phd\",3,4)\n",
        "                                          )\n",
        "                                 )\n",
        "data[\"skill_cleaned\"]=np.where(data[\"skill\"]==\"c++\",1,\n",
        "                                  np.where(data[\"skill\"]==\"java\",2,3\n",
        "                                          )\n",
        "                                 )\n",
        "data[\"enrolled_cleaned\"]=np.where(data[\"enrolled\"]==\"Yes\",1,0)\n",
        "\n",
        "# Split dataset in training and test datasets\n",
        "X_train, X_test = train_test_split(data, test_size=0.1, random_state=int(time.time()))\n",
        "\n",
        "# Instantiate the classifier\n",
        "gnb = GaussianNB()\n",
        "used_features =[\n",
        "    \"985_cleaned\",\n",
        "    \"education_cleaned\",\n",
        "    \"skill_cleaned\"\n",
        "]\n",
        "# Train classifier\n",
        "gnb.fit(\n",
        "    X_train[used_features].values,\n",
        "    X_train[\"enrolled_cleaned\"]\n",
        ")\n",
        "y_pred = gnb.predict(X_test[used_features])\n",
        "\n",
        "# Print results\n",
        "print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
        "      .format(\n",
        "          X_test.shape[0],\n",
        "          (X_test[\"enrolled_cleaned\"] != y_pred).sum(),\n",
        "          100*(1-(X_test[\"enrolled_cleaned\"] != y_pred).sum()/X_test.shape[0])\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1 points : 1, performance 00.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4oDPlRA-WQv",
        "colab_type": "text"
      },
      "source": [
        "##  逻辑回归——非线性逻辑函数的由来"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTWJX3Il_BwW",
        "colab_type": "text"
      },
      "source": [
        "### 逻辑回归"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y81usbVG_tR5",
        "colab_type": "text"
      },
      "source": [
        "*   Logistic Regression （LR），一般翻译为逻辑回归\n",
        "*   LR 是一种简单、高效的常用分类模型\n",
        "\n",
        "\n",
        "*   LR 的模型函数记作：y=h(x)\n",
        "\n",
        "*   具体形式 $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx }}$ \n",
        "\n",
        "\n",
        "\n",
        "*   对应到一元自变量的形式为：$h(x) = \\frac{1}{1 + e^{-(a + bx) }}$\n",
        "*   $z = a + bx$，则：$h(z) = \\frac{1}{1 + e^{-z }}$\n",
        "*   ![逻辑函数在二维坐标中的表现](http://images.gitbook.cn/eb56d990-51b8-11e8-bac9-5d6833199485)\n",
        "\n",
        "*   表现为 S 形曲线，所以逻辑函数又被称为 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVY6PdIf_tXx",
        "colab_type": "text"
      },
      "source": [
        "### 指数增长\n",
        "![$W(t) = ae^{bt}$](http://images.gitbook.cn/f5875700-51b8-11e8-bac9-5d6833199485)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3wVWUIv_tcK",
        "colab_type": "text"
      },
      "source": [
        "### 逻辑函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cRm8jvw_tiO",
        "colab_type": "text"
      },
      "source": [
        "#### 对指数增长的修正\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia6YdyFF_tlX",
        "colab_type": "text"
      },
      "source": [
        "*    $W'(t) = bW(t) - g(W(t))$\n",
        "*   $g(W(t))$是以  $W'(t)$ 为自变量的函数，它代表随着总数增长出现的阻力。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ZNVukuDVCU",
        "colab_type": "text"
      },
      "source": [
        "#### 修正项的具体形式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJyXKlrRDYLi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   几种不同形式的阻力（Resistance）函数，当阻力函数表现为 $W(t)$的二次形式时，新模型显示出了它的逻辑性。\n",
        "\n",
        "*   新模型的形式可以表示为：$W'(t) = bW(t)(1 – \\frac{W(t)}{L})$\n",
        "\n",
        "*   设：$P(t) = \\frac{W(t)}{L}$\n",
        "*   则有：$P'(t) = bP(t)(1 – P(t))$\n",
        "\n",
        "\n",
        "*   $P(t) = \\frac{e^{(a + bt)}}{ 1 + e^{(a + bt)}}$\n",
        "\n",
        "\n",
        "*   $P(t) = \\frac{1}{1 + e^{-(a +bt)}}$\n",
        "\n",
        "*   ![逻辑函数，在二维坐标中的形式](http://images.gitbook.cn/47b9f910-51b9-11e8-9243-af38025e3d1e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2e6B-WGDYZT",
        "colab_type": "text"
      },
      "source": [
        "#### 逻辑分布"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CFm2O_Pr9Zm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   逻辑函数表示的是存量随时间增长渐增的关系\n",
        "*   增长率与时间的关系，是存量（逻辑函数）的微分函数  ![逻辑函数的微分函数](http://images.gitbook.cn/2c701170-7375-11e8-be4d-f5aac07d9486)\n",
        "* 图形\n",
        "![逻辑函数的微分函数](http://images.gitbook.cn/57aec030-51b9-11e8-bac9-5d6833199485)\n",
        "*   辑分布函数的曲线，还有点像正态分布的倒钟形曲线\n",
        "*   逻辑分布在某些领域和场合，曾经被用来作为正态分布的替代"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPBAOuURr9lv",
        "colab_type": "text"
      },
      "source": [
        "#### 逻辑函数的通用形式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKMoK4ootcaP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   上面两幅图反映的都是 $x$为一维的情况。当 \n",
        "$x$为多维时，$a+bx$用两个向量相乘—— \n",
        "$\\theta^T x$ ——表示，于是逻辑函数就有了如下形式：\n",
        "\n",
        "\n",
        "*   $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx }}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_KY4SkUuNOQ",
        "colab_type": "text"
      },
      "source": [
        "## 追本溯源的理论学习"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxu94PaOuTar",
        "colab_type": "text"
      },
      "source": [
        "#### 以 LR 函数的形成作为实际案例，来学习借助数学工具解决问题的方法："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1igoWJeuTno",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   首先，将目标问题定义为一个函数；\n",
        "\n",
        "\n",
        "\n",
        "*   之后，选取最简单的假设作为其具体形式；\n",
        "*   然后，用事实数据验证该形式，确认有效后沿用，形成数学模型；\n",
        "\n",
        "\n",
        "*   一旦当前采用的数学模型出现问题，则对其进行修正（添加修正项），同样借助事实数据来求取修正项的形式，形成新的（升级版）数学模型。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KumkhwzOvDyh",
        "colab_type": "text"
      },
      "source": [
        "### 线性 VS 非线性"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdrX4p_ZvJ8n",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   线性：二维坐标系中的直线，三维坐标系中的平面……\n",
        "*   非线性：二位坐标中的曲线（严格的来讲，直线也是一种特殊的曲线，但为了方便而言，我们在此处用“曲”来指代“非直”。“非直”包括“弯曲”，也包括 ReLU 函数这种“一段段拼接的线段”）；三维坐标中的曲面……\n",
        "\n",
        "*   线性回归因其简单、容易理解、计算量低等特点，而在现实中得到了广泛应用（基本上，任何事情，上来先做个线性回归，即使无法得出结论，也不会消耗太多资源）\n",
        "*   有时候，一个问题用线性模型，能够得到一个七八十分、“尚且可用”的解决方案；应用非线性模型则能得到九十分，甚至九十五分的优质解决方案。但后者耗费过于巨大，导致你无法承受。很可能你宁可选一个还能用但“便宜”的，去追求性价比，而不是追求高质量。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-z1h95E5NWk",
        "colab_type": "text"
      },
      "source": [
        "## 逻辑回归——用来做分类的回归模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyHjcDwL8T6t",
        "colab_type": "text"
      },
      "source": [
        "###  回归模型做分类\n",
        "*  $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx }}$ \n",
        "*   设 $ z = \\theta^T x $ 则\n",
        "*   $h(z) = \\frac{1}{1 + e^{-z }}$\n",
        "![ S 形曲线](http://images.gitbook.cn/def9a1f0-5285-11e8-804e-bb449981bb15)\n",
        "*   当 y>0.5 时，z被归类为真（True）或阳性（Positive），否则当 y<=0.5时，z 被归类为假（False）或阴性（Negative）\n",
        "*   LR 典型的应用是二分类问题上，也就是说，把所有的数据只分为两个类。\n",
        "\n",
        "*   模型函数在 \n",
        "y\n",
        "=\n",
        "0.5\n",
        " 附近非常敏感，自变量取值稍有不同，因变量取值就会有很大差异，所以不用担心出现大量因细微特征差异而被归错类的情况——这也正是逻辑回归的“神奇”之处。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkvYY0Cs8UDB",
        "colab_type": "text"
      },
      "source": [
        "###  逻辑回归的目标函数\n",
        "\n",
        "1.   LR 的目标函数  负对数似然函数 $J(\\theta)$ \n",
        "2.   $J(\\theta) = -l(\\theta)$\n",
        "3.   对数似然函数：$l(\\theta) =  \\log(L(\\theta)) = \\sum_{i=1}^{m}[y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]$\n",
        "4.   LR 的似然函数 $L(\\theta)$\n",
        "5.   $L(\\theta) =  \\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\\theta) = \\prod_{i=1}^{m}(h_\\theta(x^{(i)}))^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}$m 个数据的联合概率\n",
        "\n",
        "*  求$J(\\theta) $的最小值\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qz8b9tu8UGe",
        "colab_type": "text"
      },
      "source": [
        "### 优化算法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_spRo1D8UJ3",
        "colab_type": "text"
      },
      "source": [
        "*   梯度下降算法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEtldgZ38UWz",
        "colab_type": "text"
      },
      "source": [
        "*  基本步骤\n",
        "\n",
        "1.   通过对 $J(\\theta)$求导获得下降方向——$J ' (\\theta)$ ；\n",
        "\n",
        "\n",
        "2.   根据预设的步长 $α$，更新参数$\\theta := \\theta − \\alpha  J’(θ)$\n",
        "\n",
        "\n",
        "3.   重复以上两步直到逼近最优值，满足终止条件。\n",
        "![梯度下降步骤](http://images.gitbook.cn/f048fa90-5286-11e8-bcd6-e300dcfa6492)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qqVi08-8Uao",
        "colab_type": "text"
      },
      "source": [
        "*   优化算法伪代码\n",
        "\n",
        "Set initial value: $\\theta_0, \\alpha$\n",
        "while (not convergence)  \n",
        "\n",
        "{  \n",
        "  $\\qquad \\theta_j := \\theta_j + \\alpha\\sum_{i=1}^{m}( y^{(i)} -  h_\\theta(x^{(i)}))x_j^{(i)}$  \n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJFTfHyPB7l_",
        "colab_type": "text"
      },
      "source": [
        "### 实例及代码实现"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arohifv0HQM0",
        "colab_type": "text"
      },
      "source": [
        "*  线性回归"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwZkNO4VG4Zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc47a3c0-11bd-48ee-e3f9-569cf8eee73d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Importing dataset\n",
        "data = pd.read_csv('quiz.csv', delimiter=',')        \n",
        "used_features = [\"Last Score\", \"Hours Spent\"]\n",
        "X = data[used_features].values\n",
        "scores = data[\"Score\"].values\n",
        "\n",
        "X_train = X[:11]\n",
        "X_test = X[11:]\n",
        "\n",
        "# Linear Regression - Regression\n",
        "y_train = scores[:11]\n",
        "y_test = scores[11:]\n",
        "\n",
        "regr = LinearRegression()\n",
        "regr.fit(X_train, y_train)\n",
        "y_predict = regr.predict(X_test)\n",
        "\n",
        "print(y_predict)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[55.33375602 54.29040467 90.76185124]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anj6p0ibJ49e",
        "colab_type": "text"
      },
      "source": [
        "*   逻辑回归"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09QfpBoyJ6n5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b3c768e9-a410-43b0-d85d-e072b7a33e8b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Importing dataset\n",
        "data = pd.read_csv('quiz.csv', delimiter=',')\n",
        "\n",
        "used_features = [ \"Last Score\", \"Hours Spent\"]\n",
        "X = data[used_features].values\n",
        "scores = data[\"Score\"].values\n",
        "\n",
        "X_train = X[:11]\n",
        "X_test = X[11:]\n",
        "\n",
        "# Logistic Regression – Binary Classification\n",
        "passed = []\n",
        "\n",
        "for i in range(len(scores)):\n",
        "    if(scores[i] >= 60):\n",
        "        passed.append(1)\n",
        "    else:\n",
        "        passed.append(0)\n",
        "\n",
        "y_train = passed[:11]\n",
        "y_test = passed[11:]\n",
        "\n",
        "classifier = LogisticRegression(C=1e5)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_predict = classifier.predict(X_test)\n",
        "print(y_predict)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWQA9OJzK_Yu",
        "colab_type": "text"
      },
      "source": [
        "### LR 处理多分类问题"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu2tZJQeMJzJ",
        "colab_type": "text"
      },
      "source": [
        "*   处理 y 的时候给它设置三个值：0 （不及格）、1（及格）和2（优秀），然后再做 LR 分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTGyMuSzMGNF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "52d91de4-468e-49d0-f764-d46cbe59ea29"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Importing dataset\n",
        "data = pd.read_csv('quiz.csv', delimiter=',')\n",
        "\n",
        "used_features = [ \"Last Score\", \"Hours Spent\"]\n",
        "X = data[used_features].values\n",
        "scores = data[\"Score\"].values\n",
        "\n",
        "X_train = X[:11]\n",
        "X_test = X[11:]\n",
        "\n",
        "# Logistic Regression - Multiple Classification\n",
        "level = []\n",
        "\n",
        "for i in range(len(scores)):\n",
        "    if(scores[i] >= 85):\n",
        "        level.append(2)\n",
        "    elif(scores[i] >= 60):\n",
        "        level.append(1)\n",
        "    else:\n",
        "        level.append(0)\n",
        "\n",
        "y_train = level[:11]\n",
        "y_test = level[11:]\n",
        "\n",
        "classifier = LogisticRegression(C=1e5)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_predict = classifier.predict(X_test)\n",
        "print(y_predict)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}