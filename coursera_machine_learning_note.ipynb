{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coursera machine learning note.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccloveak/Study_Notes/blob/master/coursera_machine_learning_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-RV1rF_FYg6",
        "colab_type": "text"
      },
      "source": [
        "#week 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpC0h-3iGZRM",
        "colab_type": "text"
      },
      "source": [
        "基本数学概念"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpkIvP-1Gh1d",
        "colab_type": "text"
      },
      "source": [
        "#week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irK8u8fYGiAO",
        "colab_type": "text"
      },
      "source": [
        "梯度下降算法及其matlab实现"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYoqP31aGwQB",
        "colab_type": "text"
      },
      "source": [
        "#week 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfsuXCJSGx__",
        "colab_type": "text"
      },
      "source": [
        "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
        "\n",
        "*  逻辑回归算法\n",
        "*   sigmoid function\n",
        "*   logistic function\n",
        "\n",
        "*   逻辑回归算法用matlab实现\n",
        "*   利用matlab自带库 计算学习效率\n",
        "\n",
        "\n",
        "*   把one vs all中的每个one拿出来单独计算\n",
        "\n",
        "\n",
        "*   自动选择 正则化函数中的   $\\lambda$\n",
        "*   高度正则化\n",
        "\n",
        "\n",
        "*   避免过度拟合\n",
        "*   $min_\\theta\\ \\dfrac{1}{2m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\ \\sum_{j=1}^n \\theta_j^2$\n",
        "\n",
        "\n",
        "\n",
        "*   $\\begin{align*} & \\text{Repeat}\\ \\lbrace \\newline & \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline & \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2...n\\rbrace\\newline & \\rbrace \\end{align*}$\n",
        "*  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   regularization parameter.\n",
        "* $  \\begin{align*}& \\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty \\newline& \\text{where}\\ \\ L = \\begin{bmatrix} 0 & & & & \\newline & 1 & & & \\newline & & 1 & & \\newline & & & \\ddots & \\newline & & & & 1 \\newline\\end{bmatrix}\\end{align*}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh-bcfogROlR",
        "colab_type": "text"
      },
      "source": [
        "# week 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bllEcCPNRRfQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   神经网络\n",
        "\n",
        "* neural network\n",
        "\n",
        "* input layer\n",
        "\n",
        "* output layer\n",
        "\n",
        "* hidden layer\n",
        "\n",
        "* activation\n",
        "\n",
        "* 前向传播\n"
      ]
    }
  ]
}